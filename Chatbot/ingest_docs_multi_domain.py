#!/usr/bin/env python3
"""
ENHANCED Multi-Domain Document Ingestion Script

T·ª± ƒë·ªông ph√¢n lo·∫°i documents v√†o c√°c domain categories:
- admission: Tuy·ªÉn sinh
- tuition: H·ªçc ph√≠
- regulations: Quy ch·∫ø ƒë√†o t·∫°o
- general: Th√¥ng tin chung

Usage:
  python3 Chatbot/ingest_docs_multi_domain.py                    # Ingest all
  python3 Chatbot/ingest_docs_multi_domain.py -l                 # List files
  python3 Chatbot/ingest_docs_multi_domain.py file1.md file2.md  # Specific files
"""
import os
import sys
import requests
import time
import argparse
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

# Fix Windows console encoding
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

# Configuration
API_URL = "http://localhost:8000/api/rag/ingest"
RAW_DIR = Path("Chatbot/assets/raw")

# Domain classification rules
DOMAIN_RULES = {
    "admission": {
        "namespace": "ptit_admission",
        "keywords": [
            "tuy·ªÉn sinh", "ƒëi·ªÉm chu·∫©n", "x√©t tuy·ªÉn", "ƒëƒÉng k√Ω x√©t tuy·ªÉn",
            "ph∆∞∆°ng th·ª©c tuy·ªÉn sinh", "ng√†nh h·ªçc", "ch·ªâ ti√™u", "h·ªì s∆° tuy·ªÉn sinh",
            "admission", "ƒëi·ªÉm x√©t tuy·ªÉn", "kh·ªëi thi", "nguy·ªán v·ªçng"
        ],
        "filename_patterns": [
            "tuyen_sinh", "tuyensinh", "admission", "diem_chuan", "diemchuan",
            "xet_tuyen", "xettuyen"
        ]
    },
    "tuition": {
        "namespace": "ptit_tuition",
        "keywords": [
            "h·ªçc ph√≠", "chi ph√≠", "l·ªá ph√≠", "h·ªçc b·ªïng", "mi·ªÖn gi·∫£m",
            "mi·ªÖn h·ªçc ph√≠", "thu ph√≠", "ƒë√≥ng h·ªçc ph√≠", "t√≠n ch·ªâ",
            "tuition", "ph√≠ ƒë√†o t·∫°o", "m·ª©c thu"
        ],
        "filename_patterns": [
            "hoc_phi", "hocphi", "tuition", "chi_phi", "chiphi",
            "hoc_bong", "hocbong", "scholarship"
        ]
    },
    "regulations": {
        "namespace": "ptit_regulations",
        "keywords": [
            "quy ch·∫ø", "quy ƒë·ªãnh", "ƒëi·ªÅu ki·ªán t·ªët nghi·ªáp", "ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o",
            "h·ªçc v·ª•", "thi c·ª≠", "ƒëi·ªÉm", "t√≠ch l≈©y t√≠n ch·ªâ", "c·∫£nh b√°o h·ªçc t·∫≠p",
            "regulations", "quy tr√¨nh", "th·ªÉ l·ªá", "n·ªôi quy"
        ],
        "filename_patterns": [
            "quy_che", "quyche", "regulation", "quy_dinh", "quydinh",
            "dao_tao", "daotao", "hoc_vu", "hocvu"
        ]
    },
    "general": {
        "namespace": "ptit_docs",
        "keywords": [],  # Fallback category
        "filename_patterns": []
    }
}


def classify_document(file_path: Path, content: str) -> Dict:
    """
    T·ª± ƒë·ªông ph√¢n lo·∫°i document v√†o domain category

    Algorithm:
    1. Check filename patterns
    2. Count keyword matches in content
    3. Return category with highest score

    Args:
        file_path: Path to document
        content: Document content

    Returns:
        Dict with category, namespace, metadata
    """
    filename_lower = file_path.name.lower()
    content_lower = content.lower()

    # Score each domain
    scores = {}

    for domain, rules in DOMAIN_RULES.items():
        if domain == "general":
            continue  # Skip general (fallback)

        score = 0

        # Check filename patterns (weight: 5 points each)
        for pattern in rules["filename_patterns"]:
            if pattern in filename_lower:
                score += 5

        # Count keyword matches in content (weight: 1 point each)
        for keyword in rules["keywords"]:
            # Count occurrences
            count = content_lower.count(keyword)
            score += count

        scores[domain] = score

    # Find domain with highest score
    if scores:
        best_domain = max(scores.items(), key=lambda x: x[1])
        if best_domain[1] > 0:  # Must have at least 1 match
            category = best_domain[0]
            namespace = DOMAIN_RULES[category]["namespace"]

            # Extract metadata based on category
            metadata = extract_metadata(category, file_path, content)

            return {
                "category": category,
                "namespace": namespace,
                "metadata": metadata,
                "score": best_domain[1]
            }

    # Fallback to general
    return {
        "category": "general",
        "namespace": "ptit_docs",
        "metadata": {"source": file_path.name},
        "score": 0
    }


def extract_metadata(category: str, file_path: Path, content: str) -> Dict:
    """
    Extract domain-specific metadata from document

    Args:
        category: Domain category
        file_path: Path to document
        content: Document content

    Returns:
        Metadata dict
    """
    metadata = {
        "source": file_path.name,
        "ingested_at": datetime.now().isoformat()
    }

    # Extract year from filename or content
    import re

    # Try filename first: "TuyenSinh_2024.md"
    year_match = re.search(r'20\d{2}', file_path.name)
    if year_match:
        metadata["year"] = year_match.group(0)
    else:
        # Try content (first occurrence)
        year_match = re.search(r'nƒÉm\s+(20\d{2})', content[:1000], re.IGNORECASE)
        if year_match:
            metadata["year"] = year_match.group(1)

    # Category-specific metadata
    if category == "admission":
        metadata["tags"] = ["tuy·ªÉn sinh"]
        if "ƒëi·ªÉm chu·∫©n" in content.lower():
            metadata["tags"].append("ƒëi·ªÉm chu·∫©n")
        if "ng√†nh h·ªçc" in content.lower():
            metadata["tags"].append("ng√†nh h·ªçc")

    elif category == "tuition":
        metadata["tags"] = ["h·ªçc ph√≠"]
        if "h·ªçc b·ªïng" in content.lower():
            metadata["tags"].append("h·ªçc b·ªïng")
        if "mi·ªÖn gi·∫£m" in content.lower():
            metadata["tags"].append("mi·ªÖn gi·∫£m")

        # Try to extract academic year
        ay_match = re.search(r'nƒÉm h·ªçc\s+(20\d{2})[-\s]*(20\d{2})?', content[:1000], re.IGNORECASE)
        if ay_match:
            if ay_match.group(2):
                metadata["academic_year"] = f"{ay_match.group(1)}-{ay_match.group(2)}"
            else:
                metadata["academic_year"] = ay_match.group(1)

    elif category == "regulations":
        metadata["tags"] = ["quy ch·∫ø"]
        if "t·ªët nghi·ªáp" in content.lower():
            metadata["tags"].append("t·ªët nghi·ªáp")
        if "ƒëi·ªÅu ki·ªán" in content.lower():
            metadata["tags"].append("ƒëi·ªÅu ki·ªán")

    return metadata


def list_available_files():
    """Li·ªát k√™ t·∫•t c·∫£ markdown files v·ªõi domain classification preview"""
    if not RAW_DIR.exists():
        print(f"‚ùå Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {RAW_DIR.absolute()}")
        return []

    md_files = sorted(list(RAW_DIR.glob("*.md")))

    if not md_files:
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file .md n√†o trong {RAW_DIR}")
        return []

    print(f"\nüìö C√≥ {len(md_files)} documents s·∫µn s√†ng:\n")
    print(f"{'No.':<4} {'Filename':<40} {'Size':<10} {'Domain (predicted)':<20}")
    print("-" * 80)

    for i, file in enumerate(md_files, 1):
        size_kb = file.stat().st_size / 1024

        # Quick classification based on filename only
        filename_lower = file.name.lower()
        predicted_domain = "general"

        for domain, rules in DOMAIN_RULES.items():
            if domain == "general":
                continue
            for pattern in rules["filename_patterns"]:
                if pattern in filename_lower:
                    predicted_domain = domain
                    break
            if predicted_domain != "general":
                break

        # Color coding
        domain_colors = {
            "admission": "üéì",
            "tuition": "üí∞",
            "regulations": "üìã",
            "general": "üìÑ"
        }
        icon = domain_colors.get(predicted_domain, "üìÑ")

        print(f"{i:<4} {file.name:<40} {size_kb:>8.1f} KB  {icon} {predicted_domain:<15}")

    print("-" * 80)
    return md_files


def read_document(file_path: Path) -> Optional[str]:
    """ƒê·ªçc n·ªôi dung document t·ª´ file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except FileNotFoundError:
        print(f"‚ùå File kh√¥ng t√¨m th·∫•y: {file_path}")
        return None
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {str(e)}")
        return None


def ingest_document(file_name: str) -> Dict:
    """
    Ingest m·ªôt document v·ªõi automatic domain classification

    Args:
        file_name: T√™n file c·∫ßn ingest

    Returns:
        Dict v·ªõi k·∫øt qu·∫£ ingest
    """
    file_path = RAW_DIR / file_name

    # Ki·ªÉm tra file t·ªìn t·∫°i
    if not file_path.exists():
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_name}")
        return {
            "success": False,
            "file": file_name,
            "error": "File not found"
        }

    # ƒê·ªçc n·ªôi dung
    content = read_document(file_path)
    if not content:
        return {
            "success": False,
            "file": file_name,
            "error": "Failed to read file"
        }

    # AUTO CLASSIFY DOMAIN
    classification = classify_document(file_path, content)

    # T·∫°o title t·ª´ filename
    title = file_name.replace('_', ' ').replace('.md', '')

    # Domain icons
    domain_icons = {
        "admission": "üéì",
        "tuition": "üí∞",
        "regulations": "üìã",
        "general": "üìÑ"
    }
    icon = domain_icons.get(classification["category"], "üìÑ")

    # Chu·∫©n b·ªã payload v·ªõi category v√† metadata
    payload = {
        "namespace_id": classification["namespace"],
        "document_title": title,
        "content": content,
        "category": classification["category"],
        "metadata": classification["metadata"]
    }

    try:
        print(f"{icon} {classification['category'].upper():<12} | {title[:40]:<40}...", end=" ", flush=True)

        response = requests.post(
            API_URL,
            json=payload,
            timeout=120
        )

        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ {result.get('chunk_count')} chunks")
            return {
                "success": True,
                "file": file_name,
                "doc_id": result.get('doc_id'),
                "chunk_count": result.get('chunk_count'),
                "category": classification["category"],
                "namespace": classification["namespace"],
                "score": classification["score"]
            }
        else:
            error_msg = response.text
            print(f"‚ùå Error {response.status_code}")
            return {
                "success": False,
                "file": file_name,
                "status_code": response.status_code,
                "error": error_msg[:100]
            }

    except requests.exceptions.ConnectionError:
        print(f"‚ùå Connection Error")
        return {
            "success": False,
            "file": file_name,
            "error": f"Cannot connect to {API_URL}"
        }
    except Exception as e:
        print(f"‚ùå {str(e)[:50]}")
        return {
            "success": False,
            "file": file_name,
            "error": str(e)
        }


def main():
    """Main ingestion flow"""
    parser = argparse.ArgumentParser(
        description="üöÄ Multi-Domain Document Ingestion with Auto-Classification",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  python3 Chatbot/ingest_docs_multi_domain.py
    ‚Üí Ingest T·∫§T C·∫¢ documents v·ªõi auto domain classification

  python3 Chatbot/ingest_docs_multi_domain.py -l
    ‚Üí Li·ªát k√™ documents v√† preview domain classification

  python3 Chatbot/ingest_docs_multi_domain.py TuyenSinh_2024.md HocPhi_2024.md
    ‚Üí Ingest specific documents
        """
    )

    parser.add_argument(
        'files',
        nargs='*',
        help='T√™n files c·∫ßn ingest (n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh s·∫Ω ingest t·∫•t c·∫£)'
    )
    parser.add_argument(
        '-l', '--list',
        action='store_true',
        help='Li·ªát k√™ t·∫•t c·∫£ documents v·ªõi domain classification preview'
    )

    args = parser.parse_args()

    # N·∫øu ch·ªçn -l, ch·ªâ li·ªát k√™
    if args.list:
        list_available_files()
        return 0

    print("=" * 80)
    print("üöÄ MULTI-DOMAIN DOCUMENT INGESTION")
    print("=" * 80)
    print(f"API URL:  {API_URL}")
    print(f"Raw Dir:  {RAW_DIR.absolute()}")
    print(f"\nDomains:  üéì admission | üí∞ tuition | üìã regulations | üìÑ general")
    print("=" * 80 + "\n")

    # Ki·ªÉm tra th∆∞ m·ª•c t·ªìn t·∫°i
    if not RAW_DIR.exists():
        print(f"‚ùå Error: Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {RAW_DIR.absolute()}")
        sys.exit(1)

    # X√°c ƒë·ªãnh files c·∫ßn ingest
    if args.files:
        files_to_ingest = args.files
    else:
        # Ingest T·∫§T C·∫¢
        all_md_files = list(RAW_DIR.glob("*.md"))
        if not all_md_files:
            print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y markdown files n√†o ƒë·ªÉ ingest")
            return 1
        files_to_ingest = [f.name for f in sorted(all_md_files)]

    if not files_to_ingest:
        print("‚ùå Kh√¥ng c√≥ files ƒë·ªÉ ingest!")
        return 1

    print(f"üìö Ingesting {len(files_to_ingest)} documents...\n")

    # Ingest documents
    results = []
    for i, file_name in enumerate(files_to_ingest, 1):
        print(f"[{i}/{len(files_to_ingest)}] ", end="")
        result = ingest_document(file_name)
        results.append(result)
        time.sleep(0.3)  # Rate limiting

    # Summary by domain
    print("\n" + "=" * 80)
    print("üìä INGESTION SUMMARY")
    print("=" * 80)

    success_count = sum(1 for r in results if r["success"])
    fail_count = len(results) - success_count
    total_chunks = sum(r.get("chunk_count", 0) for r in results if r["success"])

    # Group by domain
    by_domain = {}
    for result in results:
        if result["success"]:
            category = result.get("category", "unknown")
            if category not in by_domain:
                by_domain[category] = []
            by_domain[category].append(result)

    print(f"\nüìà By Domain:")
    for domain in ["admission", "tuition", "regulations", "general"]:
        docs = by_domain.get(domain, [])
        if docs:
            domain_icons = {"admission": "üéì", "tuition": "üí∞", "regulations": "üìã", "general": "üìÑ"}
            icon = domain_icons.get(domain, "üìÑ")
            chunks = sum(d.get("chunk_count", 0) for d in docs)
            print(f"  {icon} {domain.capitalize():<15}: {len(docs)} docs, {chunks} chunks")

    print(f"\nüìã Details:")
    for result in results:
        status = "‚úÖ" if result["success"] else "‚ùå"
        print(f"{status} {result['file']}")
        if result["success"]:
            print(f"   ‚îî‚îÄ Category: {result.get('category')}, Namespace: {result.get('namespace')}, Chunks: {result.get('chunk_count')}")
        else:
            error_msg = result.get('error', 'Unknown error')
            print(f"   ‚îî‚îÄ Error: {error_msg[:60]}")

    print("\n" + "-" * 80)
    print(f"‚úÖ Successfully ingested: {success_count}/{len(results)}")
    print(f"‚ùå Failed: {fail_count}/{len(results)}")
    print(f"üìù Total chunks created: {total_chunks}")
    print("=" * 80)

    # Return exit code
    return 0 if fail_count == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
